{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer 란?\n",
    "\n",
    "우리가 입력값으로 **입력한 정보(텍스트 혹은 이미지)를 고정된 크기의 벡터로 표현**해주는것(임베딩). 따라서 정보를 특정 방향을 가진 벡터로 표현하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roach/fintuing-course/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 특정 단어를 입력했을때 고정된 길이인 384 사이즈의 벡터를 리턴해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6660, 0.1058],\n",
       "        [0.6660, 1.0000, 0.1471],\n",
       "        [0.1058, 0.1471, 1.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(embeddings, embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩된 벡터의 유사도를 살펴보면 첫번째 문장인  \"The weather is lovely today.\" 는 두번째 문장과 \"It's so sunny outside!\" 약간 유사하다고 판단하고 있음을 확인할 수 있습니다. 반대로 세번째 문장은 첫번째 문장과 0.1058, 두번째 문장과 0.1471 인걸 보아 유사하지 않음을 확인할 수 있습니다. 이 처럼 임베딩을 통해 우리는 문장이 얼마나 의미론적으로 유사한지 판단할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've already seen this movie 은 I don't want to watch this movie again 와 0.6583706140518188 만큼 유사합니다\n",
      "I've already seen this movie 은 I like baseball 와 0.10951775312423706 만큼 유사합니다\n",
      "I've already seen this movie 은 My name is roach 와 0.22180543839931488 만큼 유사합니다\n",
      "I like soccer 은 I don't want to watch this movie again 와 0.13166044652462006 만큼 유사합니다\n",
      "I like soccer 은 I like baseball 와 0.7686097025871277 만큼 유사합니다\n",
      "I like soccer 은 My name is roach 와 0.1979486048221588 만큼 유사합니다\n",
      "he is a basketball player 은 I don't want to watch this movie again 와 0.11488835513591766 만큼 유사합니다\n",
      "he is a basketball player 은 I like baseball 와 0.2155824452638626 만큼 유사합니다\n",
      "he is a basketball player 은 My name is roach 와 0.15206508338451385 만큼 유사합니다\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\n",
    "    \"I've already seen this movie\",\n",
    "    \"I like soccer\",\n",
    "    \"he is a basketball player\"\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"I don't want to watch this movie again\",\n",
    "    \"I like baseball\",\n",
    "    \"My name is roach\"\n",
    "]\n",
    "\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n",
    "\n",
    "for i in range(0, len(embeddings1)):\n",
    "    sentence1 = sentences1[i]\n",
    "    nd1 = embeddings1[i]\n",
    "    for j in range(0, len(embeddings2)):\n",
    "        sentence2 = sentences2[j]\n",
    "        nd2 = embeddings2[j]\n",
    "        score = model.similarity(nd1, nd2)\n",
    "        print(f\"{sentence1} 은 {sentence2} 와 {score.item()} 만큼 유사합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도 기준\n",
    "\n",
    "![image.png](../image.png)\n",
    "\n",
    "유사도는 어렸을때 우리가 수학시간에 배웠던 이 점사이의 거리를 구하는 것과 비슷하다고 생각하시면 되는데요. 예를 들어, 벡터로 표현하는 것이 점을 특정 공간속에서 점을 찍는 것이라면, 저희의 유사도 기준도 가까운 점을 구하면 유사할 것이다 라고 생각할 수 있는 것이죠? 다만 벡터이기에 방향도 가지고 있어, 아래와 같이 코싸인 유사도 혹은 벡터의 내적등의 연산으로도 표현 가능합니다. 우리가 사용하는 **SentenceTransformer** 에서의 유사도 함수는 아래와 같이 4가지 사용가능합니다.\n",
    "\n",
    "- **SimilarityFunction.COSINE** : 코싸인 유사도\n",
    "- **SimilarityFunction.DOT_PRODUCT** : 내적\n",
    "- **SimilarityFunction.EUCLIDEAN** :  유클라디안 거리\n",
    "- **SimilarityFunction.MANHATTAN** :  맨하탄 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", similarity_fn_name=SimilarityFunction.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've already seen this movie 은 I don't want to watch this movie again 와 0.6583706140518188 만큼 유사합니다\n",
      "I've already seen this movie 은 I like baseball 와 0.10951775312423706 만큼 유사합니다\n",
      "I've already seen this movie 은 My name is roach 와 0.2218054234981537 만큼 유사합니다\n",
      "I like soccer 은 I don't want to watch this movie again 와 0.13166044652462006 만큼 유사합니다\n",
      "I like soccer 은 I like baseball 와 0.7686097025871277 만큼 유사합니다\n",
      "I like soccer 은 My name is roach 와 0.19794858992099762 만큼 유사합니다\n",
      "he is a basketball player 은 I don't want to watch this movie again 와 0.11488835513591766 만큼 유사합니다\n",
      "he is a basketball player 은 I like baseball 와 0.2155824452638626 만큼 유사합니다\n",
      "he is a basketball player 은 My name is roach 와 0.15206508338451385 만큼 유사합니다\n"
     ]
    }
   ],
   "source": [
    "sentences1 = [\n",
    "    \"I've already seen this movie\",\n",
    "    \"I like soccer\",\n",
    "    \"he is a basketball player\"\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"I don't want to watch this movie again\",\n",
    "    \"I like baseball\",\n",
    "    \"My name is roach\"\n",
    "]\n",
    "\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n",
    "\n",
    "for i in range(0, len(embeddings1)):\n",
    "    sentence1 = sentences1[i]\n",
    "    nd1 = embeddings1[i]\n",
    "    for j in range(0, len(embeddings2)):\n",
    "        sentence2 = sentences2[j]\n",
    "        nd2 = embeddings2[j]\n",
    "        score = model.similarity(nd1, nd2)\n",
    "        print(f\"{sentence1} 은 {sentence2} 와 {score.item()} 만큼 유사합니다\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
